# üéô Speech Emotion Recognition and Intensity Prediction using CREMA-D

## üìå Overview

This project implements Speech Emotion Recognition (SER) and Emotion Intensity Prediction using the CREMA-D dataset.

Two approaches were explored:

1Ô∏è‚É£ Classical Machine Learning using MFCC + Ensemble Methods  
2Ô∏è‚É£ Deep Learning using Fine-Tuned Wav2Vec 2.0 (Multi-task Learning)

---

## üìÇ Dataset

Dataset used: CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset)
https://www.kaggle.com/datasets/ejlok1/cremad?resource=download

- 7442 audio samples
- 6 emotion classes:
  - Angry
  - Happy
  - Sad
  - Fear
  - Disgust
  - Neutral
- 3 intensity levels:
  - Low
  - Medium
  - High

---

## üß† Approach 1: Classical ML (MFCC + Ensemble)

### Pipeline:
- Audio preprocessing (3 sec clips)
- MFCC feature extraction
- Feature scaling
- Random Forest / Ensemble classifier
- Random Forest Regressor for intensity

### Results:
- Emotion Classification Accuracy: ~50%
- Intensity MAE: ~0.17

---

## üöÄ Approach 2: Wav2Vec 2.0 Fine-Tuning

Model Used:
- facebook/wav2vec2-base (HuggingFace Transformers)

Architecture:
- Pretrained Wav2Vec2 encoder
- Classification head (Emotion)
- Regression head (Intensity)

### Results:
- Emotion Accuracy: 75% 
- Intensity MAE: 0.17

---

## ‚öôÔ∏è Installation

1. Clone repository: Upload repo in Drive(for Collab)

2. Install dependencies: pip install -r requirements.txt

3. Download and upload Creama-D Dataset in Drive

---

## ‚ñ∂ How to Run

### Option 1 ‚Äì Classical ML
Open: Emotion Recognition & Prediction(Classic ML).ipynb

Run all cells sequentially.

---

### Option 2 ‚Äì Wav2Vec 2.0
Open: Emotion & intensity prediciton (Pretrained CNN).ipynb

Enable GPU before running.

---




